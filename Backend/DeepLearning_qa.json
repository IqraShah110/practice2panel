[
    {
        "question": "What do you mean by Deep Learning?",
        "answer": "Deep Learning is nothing but a paradigm of machine learning which has shown incredible promise in recent years. This is because of the fact that Deep Learning shows a great analogy with the functioning of the neurons in the human brain."
    },
    {
        "question": "What is the difference between machine learning and deep learning?",
        "answer": "Machine learning is a field of computer science that gives computers the ability to learn without being explicitly programmed. Machine learning can be categorized in the following four categories."
    },
    {
        "question": "Supervised machine learning,",
        "answer": ""
    },
    {
        "question": "Semi-supervised machine learning,",
        "answer": ""
    },
    {
        "question": "Unsupervised machine learning,",
        "answer": ""
    },
    {
        "question": "Reinforcement learning. Deep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks. • The main difference between deep learning and machine learning is due to the way data is presented in the system. Machine learning algorithms almost always require structured data, while deep learning networks rely on layers of ANN (artificial neural networks). Machine • learning algorithms are designed to “learn” to act by understanding labeled data and then use it to produce new results with more datasets. However, when the result is incorrect, there is a need to “teach them”. Because machine learning algorithms require bulleted data, they are not suitable for solving complex queries that involve a huge amount of data. Deep • learning networks do not require human intervention, as multilevel layers in neural networks place data in a hierarchy of different concepts, which ultimately learn from their own mistakes. However, even they can be wrong if the data quality is not good enough. Data decides • everything. It is the quality of the data that ultimately determines the quality of the result. Both of these subsets of AI are somehow connected to data, which makes it possible to represent a certain • form of “intelligence.” However, you should be aware that deep learning requires much more data than a traditional machine learning algorithm. The reason for this is that deep learning networks can identify different elements in neural network layers only when more than a million data points interact. Machine learning algorithms, on the other hand, are capable of learning by pre- programmed criteria.",
        "answer": ""
    },
    {
        "question": "What, in your opinion, is the reason for the popularity of Deep Learning in recent times?",
        "answer": "Now although Deep Learning has been around for many years, the major breakthroughs from these techniques came just in recent years. This is because of two main reasons: • The increase in the amount of data generated through various sources • The growth in hardware resources required to run these models GPUs are multiple times faster and they help us build bigger and deeper deep learning models in comparatively less time than we required previously."
    },
    {
        "question": "What is reinforcement learning?",
        "answer": "Reinforcement Learning allows to take actions to max cumulative reward. It learns by trial and error through reward/penalty system. Environment rewards agent so by time agent makes better decisions. Ex: robot=agent, maze=environment. Used forcomplex tasks (self- driving cars, gameA. RL is a series of time steps in a Markov Decision Process:  Environment: space in which RL operates  State: data related to past action RL took  Action: action taken  Reward: number taken by agent after last action  Observation: data related to environment: can be visible or partially shadowed What are Artificial Neural Networks? Artificial Neural networks are a specific set of algorithms that have revolutionized machine learning. They are inspired by biological neural networks. Neural Networks can adapt to changing the input, so the network generates the best possible result without needing to redesign the output criteria. Describe the structure of Artificial Neural Networks? Artificial Neural Networks (ANNare computing systems inspired by the human brain, designed to recognize complex patterns in data. Their structure consists of layers of interconnected neurons: Input Layer: The first layer that receives raw features from the dataset. Each neuron represents a single feature and simply forwards the values. Hidden Layers: One or more layers between input and output. Each neuron computes a weighted sum of inputs, adds a bias, and applies an activation function (e.g., ReLU, Sigmoid, Tan. These nonlinear transformations allow ANNs to capture complex relationships. Output Layer: Produces the final prediction. The number of neurons depends on the task: regression (1 neuro, binary classification (1 with sigmoi, or multi-class classification (several with softma. Connections and Weights: Neurons are linked through weighted connections that determine the strength of signals. Training: ANNs learn through forward propagation (passing data to generate outputand backpropagation (adjusting weights and biases based on errors using gradient descen. In summary, ANNs consist of input, hidden, and output layers. By adjusting weights and biases during training, they learn patterns in data, making them powerful for tasks like classification, regression, and pattern recognition. How Are Weights Initialized in a Network? There are two methods here: we can either initialize the weights to zero or assign them randomly. Initializing all weights to This makes your model similar to a linear model. All the neurons and every layer perform the same operation, giving the same output and making the deep net useless. Initializing all weights randomly: Here, the weights are assigned randomly by initializing them very close to It gives better accuracy to the model since every neuron performs different computations. This is the most commonly used method. What Is the Cost Function? Also referred to as “loss” or “error,” cost function is a measure to evaluate how good your model’s performance is. It’s used to compute the error of the output layer during backpropagation. We push that error backwards through the neural network and use that during the different training functions. The most known one is the mean sum of squared errors. What Are Hyperparameters? With neural networks, you’re usually working with hyperparameters once the data is formatted correctly. Ahyperparameter is a parameter whose value is set before the learning process begins. It determines how a network is trained and the structure of the network (such as the number of hidden units, the learning rate, epochs, batches, etc.). What Will Happen If the Learning Rate Is Set inaccurately (Too Low or Too Hig? When your learning rate is too low, training of the model will progress very slowly as we are making minimal updates to the weights. It will take many updates before reaching the minimum point. If the learning rate is set too high, this causes undesirable divergent behavior to the loss function due to drastic updates in weights. It may fail to converge (model can give a good outpuor even diverge (data is too chaotic for the network to trai. What Is The Difference Between Epoch, Batch, and Iteration in Deep Learning? Epoch – Represents one iteration over the entire dataset (everything put into the training mode. • Batch – Refers to when we cannot pass the entire dataset into the neural network at once, so we • divide the dataset into several batches. Iteration – if we have 10,000 images as data and a batch size of then an epoch should run 50 iterations (10,000 divided by . What Are the Different Layers on CNN? In a Convolutional Neural Network (CN, the architecture is built from different layers, each serving a specific purpose in feature extraction and classification. The main layers are:  Input Layer Accepts the raw image data (e.g., 28×28 pixels for MNIS. Each pixel’s intensity (grayscale or RGB valueis fed into the network. •  Convolutional Layer The core building block of CNN. Applies filters/kernels (small matricethat slide over the input image to extract features like edges, textures, or shapes. Produces a feature map as output.  Activation Layer (ReLApplies a nonlinear activation function (commonly ReLU = Rectified Linear UniRemoves negative values and introduces non-linearity, allowing the network to learn complex patterns.  Pooling Layer (Subsampling/DownsamplinReduces the spatial size of feature maps to decrease computation and prevent overfitting. Common types: Max Pooling: Takes the maximum value in a region. Average Pooling: Takes the average value in a region.  Fully Connected Layer (Dense LayeFlattens the pooled feature maps into a 1D vector. Connects every neuron from the previous layer to every neuron in this layer. Functions as the decision-making part of CNN for classification.  Output Layer Produces the final prediction. Uses activation functions like: Softmax (for multi-class classificatio. Sigmoid (for binary classificatio. Summary of CNN Layers: Input → Convolution → Activation (ReL→ Pooling → Fully Connected → Output What Is Pooling on CNN, and How Does It Work? Pooling is used to reduce the spatial dimensions of a CNN. It performs down-sampling operations to reduce the dimensionality and creates a pooled feature map by sliding a filter matrix over the input matrix."
    },
    {
        "question": "What are Recurrent Neural Networks (RNNs)?",
        "answer": "Recurrent Neural Networks (RNNare a class of neural networks designed to handle sequential data, such as text, speech, and time series. Unlike traditional neural networks, RNNs have loops that allow information to persist, meaning the output at a given time depends not only on the current input but also on previous inputs. This memory makes them suitable for tasks like language modeling, translation, and speech recognition. However, standard RNNs struggle with long-term dependencies due to vanishing or exploding gradients. Variants like Long Short-Term Memory (LSTand Gated Recurrent Units (GRaddress these limitations effectively. How Does an LSTM Network Work? An LSTM (Long Short-Term Memornetwork is a special type of RNN designed to overcome the vanishing gradient problem and capture long-term dependencies in sequential data. It works by using a cell state (memorand three main gates that regulate information flow:  Forget Gate – Decides what information to discard from the cell state.  Input Gate – Determines what new information to add to the cell state.  Output Gate – Controls what information from the cell state is passed as output. This gating mechanism allows LSTMs to retain important information over long sequences while discarding irrelevant details, making them effective in tasks like language modeling, translation, and speech recognition. What Is a Multi-layer Perceptron (ML? A Multi-Layer Perceptron (MLis a class of feedforward artificial neural network composed of multiple layers of nodes. Each node (neurois connected to every neuron in the next layer, making it a fully connected network. An MLP typically includes: Input Layer – Receives raw data features. Hidden Layers – One or more layers where weighted inputs are passed through activation functions (e.g., ReLU, Sigmoid, Tanto learn complex patterns. Output Layer – Produces final predictions (classification or regressio. MLPs are the foundation of deep learning and are widely used for supervised learning tasks such as classification, regression, and pattern recognition."
    },
    {
        "question": "Explain Gradient Descent ?",
        "answer": "Gradient Descent is an optimization algorithm used to minimize a loss (errofunction in machine learning models. It works by calculating the gradient (slopof the loss function with respect to model parameters (weightand updating the parameters in the opposite direction of the gradient. The step size is controlled by a learning rate. Iteratively, this process reduces error and finds optimal weights. Variants include Batch, Stochastic (SG, and Mini-Batch Gradient Descent."
    },
    {
        "question": "What is Exploding Gradients?",
        "answer": "Exploding gradients occur when gradients grow excessively large during backpropagation, often in deep networks or RNNs. This leads to unstable training, huge weight updates, and diverging loss. Solutions: Gradient clipping, proper weight initialization, using LSTM/GRU in RNNs."
    },
    {
        "question": "What is Vanishing Gradients?",
        "answer": "Vanishing gradients happen when gradients become extremely small during backpropagation, especially in deep networks. This prevents weights from updating, slowing or stopping learning. Solutions: Use activation functions like ReLU, proper weight initialization, batch normalization, and specialized architectures like LSTMs/GRUs."
    },
    {
        "question": "What is Backpropagation and Explain How it Works ?",
        "answer": "Backpropagation is the training algorithm for neural networks. It uses the chain rule of calculus to compute gradients of the loss function with respect to each weight. Steps:  Perform a forward pass to compute predictions.  Calculate loss (erro.  Perform backward pass: propagate the error back through layers.  Update weights using Gradient Descent. This process repeats until the model converges."
    },
    {
        "question": "What are the Variants of Backpropagation?",
        "answer": "Several improved methods exist: Stochastic Backpropagation (SG– Updates weights after each sample. Mini-batch Backpropagation – Updates in small groups of samples. Batch Backpropagation – Updates after the full dataset. Adaptive Methods: Momentum (uses past gradients for smoother update. AdaGrad (adapts learning rates per paramete. RMSProp (fixes AdaGrad’s diminishing learning rat. Adam (combines Momentum + RMSPro. Would you like me to make a comparison table (side-by-sidfor exploding vs vanishing gradients so it’s easier to remember for exams/interviews? What are the different Deep Learning Frameworks? • PyTorch: PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab. It is free and open-source software released under the Modified BSD license. • TensorFlow: TensorFlow is a free and open-source software library for dataflow and differentiable programming across a range of tasks. It is a symbolic math library and is also used for machine learning applications such as neural networks. Licensed by Apache License Developed by Google Brain Team.  Microsoft Cognitive Toolkit: Microsoft Cognitive Toolkit describes neural networks as a series of computational steps via a directed graph.  Keras: Keras is an open-source neural-network library written in Python. It is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, R, Theano, or PlaidML. Designed to enable fast experimentation with deep neural networks, it focuses on being user-friendly, modular, and extensible. Licensed by MIT. What is the role of the Activation Function? The Activation function is used to introduce non-linearity into the neural network helping it to learn more complex function. Without which the neural network would be only able to learn linear function which is alinear combination of its input data. An activation function is a function in an artificial neuron that delivers an output based on inputs."
    },
    {
        "question": "Name a few Machine Learning libraries for various purposes ?",
        "answer": "For general machine learning, popular libraries include Scikit-learn, Weka, and MLlib (from Apache Spar. For deep learning, the most widely used libraries are TensorFlow, PyTorch, Keras, and MXNet. For data manipulation and analysis, developers commonly use NumPy and Pandas. For data visualization, libraries such as Matplotlib, Seaborn, and Plotly are often used. In the field of natural language processing (NL, important libraries are NLTK, SpaCy, and Hugging Face Transformers. For computer vision, popular choices include OpenCV, Purpose Libraries What is an Auto-Encoder? An Autoencoder is a type of unsupervised neural network used for dimensionality reduction, feature learning, and data compression. It has two main parts: Encoder – Compresses input data into a smaller latent (hidderepresentation. Decoder – Reconstructs the original data from the compressed representation. The network is trained to minimize the difference between the input and the reconstructed output. Autoencoders are widely used in image denoising, anomaly detection, data compression, and representation learning. Auto-encoders are simple learning networks that aim to transform inputs into outputs with the minimum possible error. This means that we want the output to be as close to input as possible. We add a couple of layers between the input and the output, and the sizes of these layers are smaller than the input layer. The auto-encoder receives unlabeled input which is then encoded to reconstruct the input. An autoencoder is a type of artificial neural network used to learn efficient data coding in an unsupervised manner. The aim of an autoencoder is to learn a representation (encodinfor a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. Several variants exist to the basic model, with the aim of forcing the learned representations of the input to assume useful properties. Autoencoders are effectively used for solving many applied problems, from face recognition to acquiring the semantic meaning of words. What is a Boltzmann Machine? Boltzmann machines have a simple learning algorithm that allows them to discover interesting features that represent complex regularities in the training data. The Boltzmann machine is basically used to optimize the weights and the quantity for the given problem. The learning algorithm is very slow in networks with many layers of feature detectors. “Restricted Boltzmann Machines” algorithm has a single layer of feature detectors which makes it faster than the rest. What Is Dropout and Batch Normalization? Dropout is a technique of dropping out hidden and visible nodes of a network randomly to prevent overfitting of data (typically dropping 20 per cent of the node. It doubles the number of iterations needed to converge the network. It used to avoid overfitting, as it increases the capacity of generalization. Batch normalization is the technique to improve the performance and stability of neural networks by normalizing the inputs in every layer so that they have mean output activation of zero and standard deviation of one. Why Is TensorFlow the Most Preferred Library in Deep Learning? TensorFlow provides both C++ and Python APIs, making it easier to work on and has a faster compilation time compared to other Deep Learning libraries like Keras and PyTorch. TensorFlow supports both CPU and GPU computing devices. What Do You Mean by Tensor in TensorFlow? Atensor is a mathematical object represented as arrays of higher dimensions. Think of a n-D matrix. These arrays of data with different dimensions and ranks fed as input to the neural network are called “Tensors.” What is the Computational Graph? Everything in a TensorFlow is based on creating a computational graph. It has a network of nodes where each node operates. Nodes represent mathematical operations, and edges represent tensors. Since data flows in the form of a graph, it is also called a “DataFlow Graph.” How is logistic regression done? Logistic regression measures the relationship between the dependent variable (our label of what we want to predicand one or more independent variables (our featureby estimating probability using its underlying logistic function (sigmoi."
    }
]